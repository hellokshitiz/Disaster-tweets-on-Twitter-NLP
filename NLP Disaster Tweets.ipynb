{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Use the official tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-17T17:29:13.764183Z","iopub.execute_input":"2021-08-17T17:29:13.764541Z","iopub.status.idle":"2021-08-17T17:29:14.612122Z","shell.execute_reply.started":"2021-08-17T17:29:13.764460Z","shell.execute_reply":"2021-08-17T17:29:14.611062Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization\n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n \nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:14.614041Z","iopub.execute_input":"2021-08-17T17:29:14.614421Z","iopub.status.idle":"2021-08-17T17:29:20.911083Z","shell.execute_reply.started":"2021-08-17T17:29:14.614380Z","shell.execute_reply":"2021-08-17T17:29:20.910115Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load training and testing data\ndf = pd.read_csv('../input/nlp-getting-started/train.csv',index_col=0)\ndf_test = pd.read_csv('../input/nlp-getting-started/test.csv',index_col=0)\ntweets = df['text']\ny = df['target']\ny = np.array(y).astype('float32')\ntweets_test = df_test['text']","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:20.914797Z","iopub.execute_input":"2021-08-17T17:29:20.915062Z","iopub.status.idle":"2021-08-17T17:29:20.999022Z","shell.execute_reply.started":"2021-08-17T17:29:20.915035Z","shell.execute_reply":"2021-08-17T17:29:20.998202Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower() # make text lower case\n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # remove URLs\n    text = re.sub('<.*?>+', '', text) # remove html tags\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    text = re.sub('\\n', '', text) # remove words conatinaing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[‘’“”…]', '', text)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:21.000586Z","iopub.execute_input":"2021-08-17T17:29:21.000912Z","iopub.status.idle":"2021-08-17T17:29:21.007478Z","shell.execute_reply.started":"2021-08-17T17:29:21.000876Z","shell.execute_reply":"2021-08-17T17:29:21.006427Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n# Applying the de=emojifying function to both test and training datasets\ntweets2 = tweets.apply(lambda x: remove_emoji(x))\ntweets_test2 = tweets_test.apply(lambda x: remove_emoji(x))","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:21.009002Z","iopub.execute_input":"2021-08-17T17:29:21.009823Z","iopub.status.idle":"2021-08-17T17:29:21.105879Z","shell.execute_reply.started":"2021-08-17T17:29:21.009778Z","shell.execute_reply":"2021-08-17T17:29:21.105091Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer_reg = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer_reg.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text\n\n# Applying the cleaning function to both test and training datasets\ntweets2 = tweets2.apply(lambda x: text_preprocessing(x))\ntweets_test2 = tweets_test2.apply(lambda x: text_preprocessing(x))\n\n# Let's take a look at the updated text\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:21.107011Z","iopub.execute_input":"2021-08-17T17:29:21.107349Z","iopub.status.idle":"2021-08-17T17:29:37.680317Z","shell.execute_reply.started":"2021-08-17T17:29:21.107314Z","shell.execute_reply":"2021-08-17T17:29:37.679450Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"id\n1    Our Deeds are the Reason of this #earthquake M...\n4               Forest fire near La Ronge Sask. Canada\n5    All residents asked to 'shelter in place' are ...\n6    13,000 people receive #wildfires evacuation or...\n7    Just got sent this photo from Ruby #Alaska as ...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"## Bag of Words Vectorizer","metadata":{}},{"cell_type":"code","source":"#count_vectorizer = CountVectorizer()\ncount_vectorizer = CountVectorizer(ngram_range = (1,1), min_df = 1)\ntrain_vectors = count_vectorizer.fit_transform(tweets2)\ntest_vectors = count_vectorizer.transform(tweets_test2)\n\n## Keeping only non-zero elements to preserve space \ntrain_vectors.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:37.681580Z","iopub.execute_input":"2021-08-17T17:29:37.682070Z","iopub.status.idle":"2021-08-17T17:29:37.868367Z","shell.execute_reply.started":"2021-08-17T17:29:37.682030Z","shell.execute_reply":"2021-08-17T17:29:37.867388Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(7613, 16412)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tf-IDF Vectorizer","metadata":{}},{"cell_type":"code","source":"tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df = 2, max_df = 0.5)\ntrain_tfidf = tfidf.fit_transform(tweets2)\ntest_tfidf = tfidf.transform(tweets_test2)\n\ntrain_tfidf.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:37.871160Z","iopub.execute_input":"2021-08-17T17:29:37.871530Z","iopub.status.idle":"2021-08-17T17:29:38.184253Z","shell.execute_reply.started":"2021-08-17T17:29:37.871475Z","shell.execute_reply":"2021-08-17T17:29:38.183100Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(7613, 11077)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Using Naives Bayes on Bag of Words","metadata":{}},{"cell_type":"code","source":"# Fitting a simple Naive Bayes on BoW\nNB_bow = MultinomialNB()\nscores = model_selection.cross_val_score(NB_bow, train_vectors, y, cv=5, scoring=\"f1\")\nscores.mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:38.186335Z","iopub.execute_input":"2021-08-17T17:29:38.186714Z","iopub.status.idle":"2021-08-17T17:29:38.224127Z","shell.execute_reply.started":"2021-08-17T17:29:38.186675Z","shell.execute_reply":"2021-08-17T17:29:38.223023Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0.6584930948850116"},"metadata":{}}]},{"cell_type":"markdown","source":"### Using Naives Bayes on TF-IDF","metadata":{}},{"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nNB_tfidf = MultinomialNB()\nscores = model_selection.cross_val_score(NB_tfidf, train_tfidf, y, cv=5, scoring=\"f1\")\nscores.mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:38.225581Z","iopub.execute_input":"2021-08-17T17:29:38.225923Z","iopub.status.idle":"2021-08-17T17:29:38.263581Z","shell.execute_reply.started":"2021-08-17T17:29:38.225886Z","shell.execute_reply":"2021-08-17T17:29:38.262508Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0.6187711183101462"},"metadata":{}}]},{"cell_type":"markdown","source":"### Naives Bayes using a Grid Search Model on Bag of Words","metadata":{}},{"cell_type":"code","source":"nb_model = MultinomialNB()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('nb', nb_model)])\n\n# parameter grid\nparam_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, \n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(train_vectors, y)  # we can use the full data here but im only using xtrain. \nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:38.264900Z","iopub.execute_input":"2021-08-17T17:29:38.265248Z","iopub.status.idle":"2021-08-17T17:29:39.951864Z","shell.execute_reply.started":"2021-08-17T17:29:38.265204Z","shell.execute_reply":"2021-08-17T17:29:39.950499Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 6 candidates, totalling 12 fits\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","output_type":"stream"},{"name":"stdout","text":"Best score: 0.731\nBest parameters set:\n\tnb__alpha: 10\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.5s\n[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    1.6s\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    1.6s finished\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Naives Bayes using a Grid Search on TF-IDF","metadata":{}},{"cell_type":"code","source":"nb_model = MultinomialNB()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('nb', nb_model)])\n\n# parameter grid\nparam_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Initialize Grid Search Model\nmodel2 = GridSearchCV(estimator=clf, param_grid=param_grid, \n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel2.fit(train_tfidf, y)  # we can use the full data here but im only using xtrain. \nprint(\"Best score: %0.3f\" % model2.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model2.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:39.953589Z","iopub.execute_input":"2021-08-17T17:29:39.954277Z","iopub.status.idle":"2021-08-17T17:29:40.067973Z","shell.execute_reply.started":"2021-08-17T17:29:39.954229Z","shell.execute_reply":"2021-08-17T17:29:40.064888Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.0157s.) Setting batch_size=2.\n[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.0362s.) Setting batch_size=4.\n","output_type":"stream"},{"name":"stdout","text":"Fitting 2 folds for each of 6 candidates, totalling 12 fits\nBest score: 0.721\nBest parameters set:\n\tnb__alpha: 1\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsubmission[\"target\"] = model.predict(test_vectors)\n\nimport os\nos.chdir('/kaggle/working')\n    \nsubmission.to_csv(\"submission1.csv\", index=False)\n\nsubmission2 = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsubmission2[\"target\"] = model2.predict(test_tfidf)\n\nsubmission2.to_csv(\"submission2.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:40.070856Z","iopub.execute_input":"2021-08-17T17:29:40.071126Z","iopub.status.idle":"2021-08-17T17:29:40.109121Z","shell.execute_reply.started":"2021-08-17T17:29:40.071097Z","shell.execute_reply":"2021-08-17T17:29:40.108210Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# USING BERT MODEL","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# The Encoding function takes the text column from train or test dataframe, the tokenizer,\n# and the maximum length of text string as input.\n\n# Outputs:\n# Tokens\n# Pad masks - BERT learns by masking certain tokens in each sequence.\n# Segment id\n\ndef bert_encode(texts, tokenizer, max_len = 512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:40.110278Z","iopub.execute_input":"2021-08-17T17:29:40.110661Z","iopub.status.idle":"2021-08-17T17:29:40.121324Z","shell.execute_reply.started":"2021-08-17T17:29:40.110619Z","shell.execute_reply":"2021-08-17T17:29:40.120300Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def build_model(bert_layer, max_len = 512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:40.122899Z","iopub.execute_input":"2021-08-17T17:29:40.123368Z","iopub.status.idle":"2021-08-17T17:29:40.136789Z","shell.execute_reply.started":"2021-08-17T17:29:40.123325Z","shell.execute_reply":"2021-08-17T17:29:40.135694Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:40.139318Z","iopub.execute_input":"2021-08-17T17:29:40.139614Z","iopub.status.idle":"2021-08-17T17:30:12.972805Z","shell.execute_reply.started":"2021-08-17T17:29:40.139588Z","shell.execute_reply":"2021-08-17T17:30:12.971934Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:30:12.974078Z","iopub.execute_input":"2021-08-17T17:30:12.974427Z","iopub.status.idle":"2021-08-17T17:30:13.091325Z","shell.execute_reply.started":"2021-08-17T17:30:12.974391Z","shell.execute_reply":"2021-08-17T17:30:13.090451Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_input = bert_encode(tweets2.values, tokenizer, max_len=160)\ntest_input = bert_encode(tweets_test2.values, tokenizer, max_len=160)\ntrain_labels = df.target.values","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:30:13.093429Z","iopub.execute_input":"2021-08-17T17:30:13.094067Z","iopub.status.idle":"2021-08-17T17:30:17.225064Z","shell.execute_reply.started":"2021-08-17T17:30:13.094025Z","shell.execute_reply":"2021-08-17T17:30:17.224216Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:30:17.226431Z","iopub.execute_input":"2021-08-17T17:30:17.226805Z","iopub.status.idle":"2021-08-17T17:30:18.453318Z","shell.execute_reply.started":"2021-08-17T17:30:17.226762Z","shell.execute_reply":"2021-08-17T17:30:18.452498Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 160)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 160)]        0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 160)]        0                                            \n__________________________________________________________________________________________________\nkeras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n__________________________________________________________________________________________________\ntf.__operators__.getitem (Slici (None, 1024)         0           keras_layer[0][1]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 1)            1025        tf.__operators__.getitem[0][0]   \n==================================================================================================\nTotal params: 335,142,914\nTrainable params: 335,142,913\nNon-trainable params: 1\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:30:18.456141Z","iopub.execute_input":"2021-08-17T17:30:18.456396Z","iopub.status.idle":"2021-08-17T17:50:15.173950Z","shell.execute_reply.started":"2021-08-17T17:30:18.456370Z","shell.execute_reply":"2021-08-17T17:50:15.173129Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/3\n381/381 [==============================] - 409s 986ms/step - loss: 0.5185 - accuracy: 0.7482 - val_loss: 0.4061 - val_accuracy: 0.8280\nEpoch 2/3\n381/381 [==============================] - 372s 977ms/step - loss: 0.3011 - accuracy: 0.8806 - val_loss: 0.4477 - val_accuracy: 0.8214\nEpoch 3/3\n381/381 [==============================] - 373s 978ms/step - loss: 0.1687 - accuracy: 0.9382 - val_loss: 0.4917 - val_accuracy: 0.8037\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}